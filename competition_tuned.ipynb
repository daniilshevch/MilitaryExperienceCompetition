{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Мета роботи: розробка моделі машинного навчання для автоматичної класифікації текстових повідомлень на предмет наявності\n",
    "ознак військового досвіду на прикладі повідомлень з Telegram-каналів\n",
    "\n",
    "Актуальність: завдання є актуальним для OSINT-аналізу та пріоритизації інформаційних потоків в реальному часі \n"
   ],
   "id": "f169e2d0513a93ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Методологія навчання: для досягнення високої точності було обрано метод ансамблювання двох моделей, що поєднує:\n",
    "1) Logistic Regression - лінійна модель для виявлення прямих лексичних залежностей\n",
    "2) Gradient Boosting - складніша модель, яка сама по собі є ансамблевою, дозволяє знаходити складні, нелінійні патерни\n",
    "\n",
    "В якості метрики оцінки локально для валідації та на змаганні на платформі Kaggle обрано метрику F1-Score як вдалий\n",
    "баланс між Precision(точністю) та Recall(повнотою)"
   ],
   "id": "d1fdfeee942863d5"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-15T11:52:53.240922Z",
     "start_time": "2025-12-15T11:52:53.228927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from xgboost import XGBClassifier"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T11:50:32.712335Z",
     "start_time": "2025-12-15T11:50:32.545329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Завантаження даних\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('to_answer.csv')\n",
    "\n",
    "#Заповнення пропусків\n",
    "train_df['cleaned_message'] = train_df['cleaned_message'].fillna('')\n",
    "test_df['cleaned_message'] = test_df['cleaned_message'].fillna('')\n",
    "\n",
    "print(f\"Розмір тренувального датасету: {train_df.shape}\")\n",
    "print(f\"Розмір тестового датасету: {test_df.shape}\")\n",
    "SPLIT_RANDOM_STATE = 35"
   ],
   "id": "c9031b31f9f97df2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Розмір тренувального датасету: (5299, 6)\n",
      "Розмір тестового датасету: (2271, 7)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "В якості препроцесингу використовується спеціальна бібліотека SnowballStemmer для російської мови. Це дозволяє зводити\n",
    "слова до їх основи(кореня) - це з одного боку зменшує розмірність словника і оптимізує процес навчання, а з іншого - \n",
    "покращує узагальнювальну здатність моделі, адже вона краще розуміє логічний сенс та спорідненість слів.\n",
    "Також застосовується Regex-фільтр(на основі регулярних виразів) для залишення лише кирилииці та латиниці"
   ],
   "id": "dfc7156c0f56e741"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T11:50:33.051651Z",
     "start_time": "2025-12-15T11:50:33.043756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stemmer = SnowballStemmer(\"russian\")\n",
    "def stemming_tokenizer(text):\n",
    "    tokens = re.findall(r'(?u)\\b\\w\\w+\\b', text.lower())\n",
    "    return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "X = train_df['cleaned_message']\n",
    "y = train_df['new_label']"
   ],
   "id": "24c8bada6f932be6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T11:50:33.085017Z",
     "start_time": "2025-12-15T11:50:33.070614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Ділимо датасет на аргументи та цільову змінну\n",
    "X = train_df['cleaned_message']\n",
    "y = train_df['new_label']\n",
    "#Проводимо поділ датасету на тренувальний та валідаціний набір в співвдошенні 4:1\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SPLIT_RANDOM_STATE, stratify=y)"
   ],
   "id": "7461c2c6ac181b6c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Далі використовується векторизація(аспект Feature Engineering) через TF-IDF Vectorizer\n",
    "(Term Frequency - Inverse Document Frequency). Використовується з параметрами:\n",
    "1) ngram_range = (1,3) - враховуються не тільки окремі слова, а й фрази з 2-3 слів, що дозволяє частково вловлювати\n",
    "контекст словосполучень та речень\n",
    "2) max_features = 50000 - обмеження словника найбільш значущими токенами "
   ],
   "id": "1871c3f98449421"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T11:50:50.288990Z",
     "start_time": "2025-12-15T11:50:33.109215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Проводимо векторизацію\n",
    "print(\"Векторизація\")\n",
    "tfidf = TfidfVectorizer(max_features=50000, ngram_range=(1, 3), tokenizer=stemming_tokenizer, token_pattern=None)\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_val_vec = tfidf.transform(X_val)"
   ],
   "id": "a96236202d78134d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизація\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Далі після первинної підготовки даних створюємо ансамбль для навчання, який буде працювати за принципом\n",
    "голосування(Voting Ensemble). Такий підхід дозволяє компенсувати слабкі сторони однієї моделі сильними\n",
    "сторонами іншої, зменшуючи загальну дисперсію помилки.\n",
    "Для нас важливо підібрати оптимальні гіперпараметри для моделей та ансамблю, тому ми спочатку будемо\n",
    "проводити ручний первинний GridSearch для того, щоб подивитись, яка комбінація параметрів дозволяє \n",
    "досягнути найвищого F1-Score на валідації"
   ],
   "id": "19b91ce07d718acf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Перша модель - XgBoost Classifier(Градієнтний бустинг). Градієнтний бустинг використовується для виявлення складних\n",
    "нелінійних патернів та взаємозв'язків між словами (feature interactions), які не може вловити логістична регресія.\n",
    "Модель будує ансамбль дерев рішень послідовно, виправляючи помилки попередніх дерев. Параметри:\n",
    "1) scale_pos_weight=ratio - Cпеціальний параметр для роботи з незбалансованими даними в XGBoost.\n",
    "    Розрахунок: sum(negative instances) / sum(positive instances).\n",
    "    Дія: Збільшує вагу градієнта для позитивного класу, змушуючи модель приділяти більше уваги \"рідкісним\" приклад\n",
    "    (військовий досвід/загроза).\n",
    "2) n_estimators = 200 - Кількість дерев в ансамблі. Значення підібрано емпірично як баланс між здатністю до навчання]\n",
    "та часом тренування.\n",
    "3) max_depth = 6 - Максимальна глибина одного дерева. Глибина 6 дозволяє моделювати взаємодії між групами слів\n",
    "(контекст), але запобігає надмірному перенавчанню, яке виникає при глибоких деревах.\n",
    "4) learning_rate = 0.1 - Швидкість навчання. Стандартне значення, яке забезпечує стабільну збіжність при заданій\n",
    "кількості дерев\n",
    "5) n_jobs = -1 - використання всіх доступних ядер процесора\n",
    "6) eval_metric = \"logloss\" - метрика оцінки якості під час навчання, використовує Logarithmic Loss - стандартну\n",
    "функцію втрат для бінарної класифікації.На відміну від простої точності (Accuracy), logloss враховує впевненість\n",
    "моделі у прогнозі. Модель отримує більший штраф, якщо вона впевнено передбачає неправильний клас. Це змушує алгоритм\n",
    "калібрувати ймовірності, а не просто вгадувати мітки 0 чи 1.\n",
    "\n",
    "В даній роботі я не проводжу оптимізацію гіперпараметрів моделі Gradient Boosting, тому дана модель \n",
    "створюється і навчається один раз, оскільки немає потреби проганяти її через цикл."
   ],
   "id": "e8c135a442da799c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T11:51:04.884942Z",
     "start_time": "2025-12-15T11:50:50.323278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ratio = float(np.sum(y == 0)) / np.sum(y == 1) \n",
    "clf_xgb = XGBClassifier(\n",
    "    scale_pos_weight=ratio,\n",
    "    n_estimators=200, \n",
    "    max_depth=6, \n",
    "    learning_rate=0.1,\n",
    "    random_state=44, \n",
    "    n_jobs=-1, \n",
    "    eval_metric='logloss'\n",
    ")\n",
    "clf_xgb.fit(X_train_vec, y_train)\n",
    "xgb_preds_val = clf_xgb.predict_proba(X_val_vec)[:, 1]"
   ],
   "id": "7e187bd1f4f4dfc4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Друга модель - логістична регресія. Логістична регресія була обрана як базовий алгоритм (baseline), оскільки вона\n",
    "демонструє високу ефективність на розріджених даних високої розмірності (TF-IDF матриці). Вона відмінно знаходить\n",
    "лінійні залежності між наявністю певних токенів та цільовим класом.\n",
    "Параметри:\n",
    "1) class_weight = \"balanced\" - Автоматичне балансування ваг класів обернено пропорційно їх частоті. Це критично важливо для максимізації метрики F1, оскільки штраф за помилку на меншості (клас 1) стає вищим.\n",
    "2) max_iter = 2000 - Збільшена кількість ітерацій солвера. Оскільки матриця ознак дуже велика, стандартних 100\n",
    "ітерацій часто недостатньо для збіжності градієнтного спуску\n",
    "3) n_jobs = -1 - використання всіх доступних ядер процесора\n",
    "4) C = [1,2,3,4,5,6,7,10] - цей параметр змінюється в кожній новій моделі логістичної регресії,\n",
    "яку ми утворюємо при проході по циклу. Чим більше цей параметр, тим сильніше\n",
    "моделі дозволено підлаштовуватися під тренувальні дані, що виправдано при великому \n",
    "словнику (50k ознак), де важливі рідкісні, але \"сильні\" слова"
   ],
   "id": "39ff5e662c01fb6e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Далі ми проводимо ручний Grid Search, де перебираємо параметри коефіцієнта C оберненої регуляризації для \n",
    "логістичної регресії(перший зовнішній цикл), ваги моделей в ансамблі(другий зовнішній цикл) та показники\n",
    "порогу(threshold)(третій цикл, внутрішній). Перебираючи ці комбінації, рахуємо F1-Score на валідаційному\n",
    "наборі і визначаємо оптимальну комбінацію."
   ],
   "id": "ac19ce57a473120"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T11:55:12.425702Z",
     "start_time": "2025-12-15T11:55:12.413820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Перебираємо параметр C для логістичної регресії\n",
    "c_values = [1.0, 2.0, 3.0, 4.0, 5.0, 7.0, 10.0]\n",
    "# Перебираємо ваги для ансамблю (LR, XGB)\n",
    "weight_options = [(1, 1), (1.5, 1), (2, 1), (1, 1.5), (1, 2)]\n",
    "best_overall_f1 = 0\n",
    "best_params = {}"
   ],
   "id": "5729d4116f7db91c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T11:51:29.679156Z",
     "start_time": "2025-12-15T11:51:04.961768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for c_val in c_values:\n",
    "    # Тренуємо LogisticRegression з новим C\n",
    "    clf_lr_temp = LogisticRegression(\n",
    "        C=c_val, \n",
    "        class_weight='balanced', \n",
    "        max_iter=2000, \n",
    "        random_state=44, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    #Тренуємо модель логістичної регресії з заданими гіперпараметрами\n",
    "    clf_lr_temp.fit(X_train_vec, y_train)\n",
    "    #Передбачаємо ймовірності появи обох класів для кожного з семплів і запам'ятовуємо\n",
    "    #ймовірність появи класу 1\n",
    "    lr_preds_val = clf_lr_temp.predict_proba(X_val_vec)[:, 1]\n",
    "    \n",
    "    # Перебираємо ваги для кожної з двох моделей ансамблю\n",
    "    for w_lr, w_xgb in weight_options:\n",
    "        # Ручне ансамблювання ймовірностей (швидше, ніж VotingClassifier)\n",
    "        ensemble_preds = (lr_preds_val * w_lr + xgb_preds_val * w_xgb) / (w_lr + w_xgb)\n",
    "        \n",
    "        # Підбір порогу для цієї комбінації\n",
    "        current_best_f1 = 0\n",
    "        current_thresh = 0.5\n",
    "        \n",
    "        # Перебір різного рівня порогу(threshold) прийняття рішень\n",
    "        # для класифікації як клас 1\n",
    "        for threshold in np.arange(0.2, 0.8, 0.05):\n",
    "            score = f1_score(y_val, (ensemble_preds >= threshold).astype(int))\n",
    "            if score > current_best_f1:\n",
    "                current_best_f1 = score\n",
    "                current_thresh = threshold\n",
    "        \n",
    "        # Якщо ми змогли знайти кращу комбінацію гіперпараметрів(з вищим F1-Score), то \n",
    "        # запам'ятовуємо ці параметри\n",
    "        if current_best_f1 > best_overall_f1:\n",
    "            best_overall_f1 = current_best_f1\n",
    "            best_params = {\n",
    "                'C': c_val,\n",
    "                'weights': (w_lr, w_xgb),\n",
    "                'threshold': current_thresh\n",
    "            }\n",
    "            print(f\"New best! F1: {best_overall_f1:.4f} | C={c_val}, Weights={w_lr}:{w_xgb}, Thresh={current_thresh:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"НАЙКРАЩИЙ РЕЗУЛЬТАТ: F1 = {best_overall_f1:.4f}\")\n",
    "print(f\"Параметри: {best_params}\")\n",
    "print(\"=\"*30)"
   ],
   "id": "5e57b9b36ecd7a25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best! F1: 0.7903 | C=1.0, Weights=1:1, Thresh=0.55\n",
      "New best! F1: 0.7921 | C=1.0, Weights=1.5:1, Thresh=0.50\n",
      "New best! F1: 0.7952 | C=1.0, Weights=2:1, Thresh=0.55\n",
      "New best! F1: 0.8000 | C=2.0, Weights=1:1, Thresh=0.55\n",
      "New best! F1: 0.8069 | C=2.0, Weights=1.5:1, Thresh=0.55\n",
      "New best! F1: 0.8131 | C=3.0, Weights=1.5:1, Thresh=0.55\n",
      "New best! F1: 0.8139 | C=3.0, Weights=2:1, Thresh=0.55\n",
      "New best! F1: 0.8166 | C=4.0, Weights=1.5:1, Thresh=0.55\n",
      "New best! F1: 0.8180 | C=4.0, Weights=2:1, Thresh=0.55\n",
      "New best! F1: 0.8188 | C=5.0, Weights=2:1, Thresh=0.55\n",
      "New best! F1: 0.8209 | C=7.0, Weights=1.5:1, Thresh=0.55\n",
      "New best! F1: 0.8209 | C=10.0, Weights=1.5:1, Thresh=0.50\n",
      "\n",
      "==============================\n",
      "НАЙКРАЩИЙ РЕЗУЛЬТАТ: F1 = 0.8209\n",
      "Параметри: {'C': 10.0, 'weights': (1.5, 1), 'threshold': np.float64(0.49999999999999994)}\n",
      "==============================\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Після підбору оптимальних гіперпараметрів проводимо фінальне навчання ансамблю методом \n",
    "Soft Voting, застосовуючи вже повноцінний VotingClassifier з scikit-learn. "
   ],
   "id": "fb631882a4ccda22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T11:52:20.728779Z",
     "start_time": "2025-12-15T11:51:29.732404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. ФІНАЛЬНЕ ТРЕНУВАННЯ З НАЙКРАЩИМИ ПАРАМЕТРАМИ\n",
    "print(\"Фінальне тренування...\")\n",
    "\n",
    "# Виконуємо TF-IDF векторизацію на всьому датасеті\n",
    "X_full_vec = tfidf.fit_transform(X)\n",
    "X_test_vec = tfidf.transform(test_df['cleaned_message'])\n",
    "\n",
    "# Тренуємо логістичну регресію з оптимальним коефіцієнтом оберненої регуляризації C\n",
    "final_lr = LogisticRegression(\n",
    "    C=best_params['C'], \n",
    "    class_weight='balanced', \n",
    "    max_iter=3000, \n",
    "    random_state=44, \n",
    "    n_jobs=-1\n",
    ")\n",
    "# Тренуємо XGBoost\n",
    "final_xgb = XGBClassifier(\n",
    "    n_estimators=200, \n",
    "    max_depth=6, \n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=ratio, \n",
    "    random_state=44, \n",
    "    n_jobs=-1, \n",
    "    eval_metric='logloss'\n",
    ")\n",
    "# Створюємо ансамбль моделей, що працюють за принципом \"Soft Voting\"\n",
    "voting_final = VotingClassifier(\n",
    "    estimators=[('lr', final_lr), ('xgb', final_xgb)],\n",
    "    voting='soft',\n",
    "    weights=list(best_params['weights'])\n",
    ")\n",
    "#Тренуємо ансамбль\n",
    "voting_final.fit(X_full_vec, y)\n",
    "# Перевіряємо прогнози ансамблю і обробляємо їх відповідно до нашого оптимального\n",
    "# порогу(threshold)\n",
    "test_proba = voting_final.predict_proba(X_test_vec)[:, 1]\n",
    "test_predictions = (test_proba >= best_params['threshold']).astype(int)\n",
    "\n",
    "# Створюємо фінальний submission\n",
    "submission = pd.DataFrame({\n",
    "    \"row ID\": test_df[\"row ID\"],\n",
    "    \"new_label\": test_predictions\n",
    "})\n",
    "filename = f\"submission_tuned_C{best_params['C']}_W{best_params['weights'][0]}-{best_params['weights'][1]}.csv\"\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"Файл збережено: {filename}\")"
   ],
   "id": "16e8fef1bab8bf90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Фінальне тренування...\n",
      "Файл збережено: submission_tuned_C10.0_W1.5-1.csv\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Висновки: отже, для даного змагання на Kaggle, було імплеметовано ансамблевий підхід прогнозування, \n",
    "застосовано поєднання логістичної регресії та моделі XGBoost. Через пошук по сітці(Grid Search) було\n",
    "підібрано оптимальну комбінацію гіперпараметрів, яка забезпечила найкраший результат на валідаційному\n",
    "наборі. Гіперпараметри підбирались для моделі логістичної регресії, а також для балансування важливості\n",
    "голосів моделей в ансамблі."
   ],
   "id": "84f484ee1a281612"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
